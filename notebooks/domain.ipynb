{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f01589-f035-4d04-922a-c9c6495ad117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from typing import Tuple, Dict, List, Any\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler, Optimizer\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional.classification import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a11ee339-dcd1-46f2-95d5-edca25fe5d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Block(nn.Module):\n",
    "    \"\"\"Building block for MLP-based models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_size: int, activation: nn.Module, depth: int\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the MLP block.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Number of neurons in the linear layer.\n",
    "            activation: Activation function.\n",
    "            depth: Number of MLP blocks (linear layer with activation).\n",
    "        \"\"\"\n",
    "        super(MLP_Block, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            linear = nn.Linear(hidden_size, hidden_size)\n",
    "            layers.append(linear)\n",
    "            layers.append(activation)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates the input through the MLP block.\n",
    "\n",
    "        Args:\n",
    "            x: Input.\n",
    "\n",
    "        Returns:\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int],\n",
    "        output_shape: Tuple[int],\n",
    "        hidden_factor: int = 1,\n",
    "        depth: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the multi-layer perceptron.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input.\n",
    "            output_shape: Shape of the output.\n",
    "            hidden_factor: Factor for multiplying with input length to\n",
    "                determine the number of neurons in each hidden layer.\n",
    "                Defaults to 1.\n",
    "            depth: Number of hidden layers. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        input_len = int(np.prod(input_shape))\n",
    "        output_len = int(np.prod(output_shape))\n",
    "        hidden_size = int(input_len * hidden_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [   \n",
    "                nn.Flatten(),\n",
    "                nn.Linear(input_len, hidden_size),  # Input layer\n",
    "                MLP_Block(hidden_size, nn.ReLU(), depth),\n",
    "                nn.Linear(hidden_size, output_len),  # Output layer\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates the input through the MLP block.\n",
    "\n",
    "        Args:\n",
    "            x: Input.\n",
    "\n",
    "        Returns:\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "061442de-fc7e-4e6e-857d-5791aefaf74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitClassificationModel(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: str,\n",
    "        lr: float,\n",
    "        num_classes: int,\n",
    "        criterion,\n",
    "        optimizer_class,\n",
    "        step_size,\n",
    "        scheduler_class,\n",
    "        ) -> None:\n",
    "        \"\"\"Initialization of the custom Lightning Module.\n",
    "\n",
    "        Args:\n",
    "            model: Neural network model name.\n",
    "            config: Neural network model and training config.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.step_size = step_size\n",
    "        self.scheduler_class = scheduler_class\n",
    "        self.net = net\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Tuple[Optimizer, lr_scheduler.LRScheduler]:\n",
    "        \"\"\"Configures the optimizer and scheduler based on the learning rate\n",
    "            and step size.\n",
    "\n",
    "        Returns:\n",
    "            Configured optimizer and scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizer_class(self.parameters(), lr=self.lr)\n",
    "        scheduler = self.scheduler_class(optimizer, self.step_size)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def infer_batch(\n",
    "        self, batch: Dict[str, dict]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Propagate given batch through the Lightning Module.\n",
    "\n",
    "        Args:\n",
    "            batch: Batch containing the subjects.\n",
    "\n",
    "        Returns:\n",
    "            Model output and corresponding ground truth.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    def training_step(self, batch: Dict[str, dict], batch_idx: int) -> float:\n",
    "        \"\"\"Infer batch on training data, log metrics and retrieve loss.\n",
    "\n",
    "        Args:\n",
    "            batch: Batch containing the subjects.\n",
    "            batch_idx: Number displaying index of this batch.\n",
    "\n",
    "        Returns:\n",
    "            Calculated loss.\n",
    "        \"\"\"\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc =  multiclass_accuracy(y_hat, y, num_classes=self.num_classes)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('acc', acc, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2b40019-76fa-4dfe-88c2-d4057b844003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleFreqSpace(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, img):\n",
    "        return torch.fft.rfft2(img)\n",
    "\n",
    "\n",
    "class SimpleComplex2Vec(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.view_as_real(x)\n",
    "\n",
    "class BaseDataModule(L.LightningDataModule):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Creates Dataloader for training phase.\n",
    "\n",
    "        Returns:\n",
    "            Dataloader for training phase.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Creates Dataloader for validation phase.\n",
    "\n",
    "        Returns:\n",
    "            Dataloader for validation phase.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, self.batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class ImageNetDataModule(BaseDataModule):\n",
    "    def __init__(self, data_dir: str, input_domain: str, batch_size: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.input_domain = input_domain\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        traindir = os.path.join(self.data_dir, 'train')\n",
    "        valdir = os.path.join(self.data_dir, 'val')\n",
    "\n",
    "        if self.input_domain == 'freq':\n",
    "            domain_transfrom = [SimpleFreqSpace(), SimpleComplex2Vec()]\n",
    "        else:\n",
    "            domain_transform = [lambda x: x]\n",
    "\n",
    "        self.train_set = ImageFolder(\n",
    "            traindir,\n",
    "            transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    *domain_transfrom,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.val_set = ImageFolder(\n",
    "            valdir,\n",
    "            transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    *domain_transfrom,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class MNISTDataModule(BaseDataModule):\n",
    "    \n",
    "    def __init__(self, input_domain: str, batch_size: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.input_domain = input_domain\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        datasets.MNIST(\n",
    "            root=\"MNIST\", download=True, train=False)\n",
    "        datasets.MNIST(\n",
    "            root=\"MNIST\", download=True, train=False)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        tensor_transform = transforms.ToTensor()\n",
    "        \n",
    "        if self.input_domain == 'freq':\n",
    "            domain_transfrom = [SimpleFreqSpace(), SimpleComplex2Vec()]\n",
    "        else:\n",
    "            domain_transform = [lambda x: x]\n",
    "\n",
    "        self.test_set = datasets.MNIST(\n",
    "            root=\"MNIST\", download=True, train=False, transform= transforms.Compose([tensor_transform ]))\n",
    "        \n",
    "        # use 20% of training data for validation\n",
    "        train_set_size = int(len(train_set) * 0.8)\n",
    "        valid_set_size = len(train_set) - train_set_size\n",
    "        \n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(\n",
    "            train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "        \n",
    "        self.test_set = datasets.MNIST(\n",
    "            root=\"MNIST\", download=True, train=False, transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fcb4bf2d-97c1-499d-bdec-99a27eff30a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTDataModule2(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = \"./\"):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        datasets.MNIST(self.data_dir, train=True, download=True)\n",
    "        datasets.MNIST(self.data_dir, train=False, download=True)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == \"fit\":\n",
    "            mnist_full = datasets.MNIST(self.data_dir, train=True, transform=self.transform)\n",
    "            self.mnist_train, self.mnist_val = torch.utils.data.random_split(\n",
    "                mnist_full, [55000, 5000], generator=torch.Generator().manual_seed(42)\n",
    "            )\n",
    "\n",
    "        # Assign test dataset for use in dataloader(s)\n",
    "        if stage == \"test\":\n",
    "            self.mnist_test = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.mnist_predict = datasets.MNIST(self.data_dir, train=False, transform=self.transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.mnist_train, batch_size=32)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.mnist_val, batch_size=32)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.mnist_test, batch_size=32)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.mnist_predict, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "70b5f892-0e60-42e8-98b9-c809cb931f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import  defaultdict\n",
    "class FindSumPairs:\n",
    "\n",
    "    def __init__(self, nums1: List[int], nums2: List[int]):\n",
    "        self.dict1 = defaultdict(int)\n",
    "        for num in nums1:\n",
    "            self.dict1[num] += 1\n",
    "        self.dict2 = defaultdict(int)\n",
    "        for num in nums2:\n",
    "            self.dict2[num] += 1\n",
    "        self.nums2 = nums2\n",
    "\n",
    "    def add(self, index: int, val: int) -> None:\n",
    "        print(self.dict2)\n",
    "        self.dict2[self.nums2[index] + val] += 1\n",
    "        self.dict2[self.nums2[index]] -= 1 \n",
    "\n",
    "    def count(self, tot: int) -> int:\n",
    "        print(self.dict2)\n",
    "        ret = 0\n",
    "        for num, count in self.dict1.items():\n",
    "            ret += (count * self.dict2[tot - num])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3e7de73f-7ebb-444d-9f7b-33fb333e7583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = torch.utils.data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3d5e9961-96b9-43bd-8caf-e92dc52d0d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = MLP([28, 28], [10])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.0001\n",
    "num_classes = 10\n",
    "step_size = 300\n",
    "scheduler_class = torch.optim.lr_scheduler.StepLR\n",
    "optimizer_class = torch.optim.Adam\n",
    "lit_model = LitClassificationModel(\n",
    "    net, lr, num_classes, criterion,\n",
    "    optimizer_class, step_size, scheduler_class\n",
    ")\n",
    "datamodule = MNISTDataModule2()#('freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d6affa9-a35d-4a4e-8dff-48a8ca830016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | net       | MLP              | 1.2 M \n",
      "-----------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.955     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1719/1719 [00:32<00:00, 52.26it/s, v_num=45, train_loss=0.0182]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1719/1719 [00:32<00:00, 52.22it/s, v_num=45, train_loss=0.0182]\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=6)\n",
    "trainer.fit(model=lit_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ce6d81da-4bc2-40a8-a020-348de8cea8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:27<00:00, 367.32it/s]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           acc              0.9675999879837036\n",
      "        test_loss            0.355621337890625\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.355621337890625, 'acc': 0.9675999879837036}]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=lit_model, dataloaders=DataLoader(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f22526d-7cc9-4932-a0b7-7b696cd7b9a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "domain",
   "language": "python",
   "name": "domain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
