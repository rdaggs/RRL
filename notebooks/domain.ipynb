{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08f01589-f035-4d04-922a-c9c6495ad117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import Tuple, Dict, List, Any\n",
    "\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler, Optimizer\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.functional.classification import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11ee339-dcd1-46f2-95d5-edca25fe5d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MLP_Block(nn.Module):\n",
    "    \"\"\"Building block for MLP-based models.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_size: int, activation: nn.Module, depth: int\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the MLP block.\n",
    "\n",
    "        Args:\n",
    "            hidden_size: Number of neurons in the linear layer.\n",
    "            activation: Activation function.\n",
    "            depth: Number of MLP blocks (linear layer with activation).\n",
    "        \"\"\"\n",
    "        super(MLP_Block, self).__init__()\n",
    "        layers = []\n",
    "        for _ in range(depth):\n",
    "            linear = nn.Linear(hidden_size, hidden_size)\n",
    "            layers.append(linear)\n",
    "            layers.append(activation)\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates the input through the MLP block.\n",
    "\n",
    "        Args:\n",
    "            x: Input.\n",
    "\n",
    "        Returns:\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape: Tuple[int],\n",
    "        output_shape: Tuple[int],\n",
    "        hidden_factor: int = 1,\n",
    "        depth: int = 1,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialization of the multi-layer perceptron.\n",
    "\n",
    "        Args:\n",
    "            input_shape: Shape of the input.\n",
    "            output_shape: Shape of the output.\n",
    "            hidden_factor: Factor for multiplying with input length to\n",
    "                determine the number of neurons in each hidden layer.\n",
    "                Defaults to 1.\n",
    "            depth: Number of hidden layers. Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        input_len = int(np.prod(input_shape))\n",
    "        output_len = int(np.prod(output_shape))\n",
    "        hidden_size = int(input_len * hidden_factor)\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [   \n",
    "                nn.Flatten(),\n",
    "                nn.Linear(input_len, hidden_size),  # Input layer\n",
    "                MLP_Block(hidden_size, nn.ReLU(), depth),\n",
    "                nn.Linear(hidden_size, output_len),  # Output layer\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Propagates the input through the MLP block.\n",
    "\n",
    "        Args:\n",
    "            x: Input.\n",
    "\n",
    "        Returns:\n",
    "            Output of the network.\n",
    "        \"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061442de-fc7e-4e6e-857d-5791aefaf74b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitClassificationModel(L.LightningModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        net: str,\n",
    "        lr: float,\n",
    "        num_classes: int,\n",
    "        criterion,\n",
    "        optimizer_class,\n",
    "        step_size,\n",
    "        scheduler_class,\n",
    "        ) -> None:\n",
    "        \"\"\"Initialization of the custom Lightning Module.\n",
    "\n",
    "        Args:\n",
    "            model: Neural network model name.\n",
    "            config: Neural network model and training config.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.step_size = step_size\n",
    "        self.scheduler_class = scheduler_class\n",
    "        self.net = net\n",
    "\n",
    "    def configure_optimizers(\n",
    "        self,\n",
    "    ) -> Tuple[Optimizer, lr_scheduler.LRScheduler]:\n",
    "        \"\"\"Configures the optimizer and scheduler based on the learning rate\n",
    "            and step size.\n",
    "\n",
    "        Returns:\n",
    "            Configured optimizer and scheduler.\n",
    "        \"\"\"\n",
    "        optimizer = self.optimizer_class(self.parameters(), lr=self.lr)\n",
    "        scheduler = self.scheduler_class(optimizer, self.step_size)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def infer_batch(\n",
    "        self, batch: Dict[str, dict]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Propagate given batch through the Lightning Module.\n",
    "\n",
    "        Args:\n",
    "            batch: Batch containing the subjects.\n",
    "\n",
    "        Returns:\n",
    "            Model output and corresponding ground truth.\n",
    "        \"\"\"\n",
    "        x, y = batch\n",
    "        y_hat = self.net(x)\n",
    "        return y_hat, y\n",
    "\n",
    "    def training_step(self, batch: Dict[str, dict], batch_idx: int) -> float:\n",
    "        \"\"\"Infer batch on training data, log metrics and retrieve loss.\n",
    "\n",
    "        Args:\n",
    "            batch: Batch containing the subjects.\n",
    "            batch_idx: Number displaying index of this batch.\n",
    "\n",
    "        Returns:\n",
    "            Calculated loss.\n",
    "        \"\"\"\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # this is the test loop\n",
    "        y_hat, y = self.infer_batch(batch)\n",
    "        loss = self.criterion(y_hat, y)\n",
    "        acc =  multiclass_accuracy(y_hat, y, num_classes=self.num_classes)\n",
    "        self.log('test_loss', loss, prog_bar=True)\n",
    "        self.log('acc', acc, prog_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b40019-76fa-4dfe-88c2-d4057b844003",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SimpleFreqSpace(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, img):\n",
    "        return torch.fft.rfft2(img)\n",
    "\n",
    "\n",
    "class SimpleComplex2Vec(torch.nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, m = x.shape[-2], x.shape[-1]\n",
    "        return torch.cat(\n",
    "            [\n",
    "                torch.stack(\n",
    "                [\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            x[:, : n // 2 + 1, 0:1].real,\n",
    "                            x[:, 1 : (n + 1) // 2, 0:1].imag\n",
    "                        ], 1),\n",
    "                    torch.cat(\n",
    "                        [\n",
    "                            x[:, : n // 2 + 1, m - 1 : m].real,\n",
    "                            x[:, 1 : (n + 1) // 2, m - 1 : m].imag\n",
    "                        ], 1)],\n",
    "                dim=3),\n",
    "              torch.view_as_real(x[:, :, 1:-1])],\n",
    "            dim=2)\n",
    "\n",
    "class BaseDataModule(L.LightningDataModule):\n",
    "    \n",
    "    def __init__(self, domain: str):\n",
    "        super().__init__()\n",
    "        self.domain = domain\n",
    "\n",
    "        if self.domain == 'freq':\n",
    "            self.domain_transform = transforms.Compose([SimpleFreqSpace(), SimpleComplex2Vec()])\n",
    "        else:\n",
    "            self.domain_transform = torch.nn.Identity()\n",
    "        \n",
    "\n",
    "    def train_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Creates Dataloader for training phase.\n",
    "\n",
    "        Returns:\n",
    "            Dataloader for training phase.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_set, self.batch_size\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> torch.utils.data.DataLoader:\n",
    "        \"\"\"Creates Dataloader for validation phase.\n",
    "\n",
    "        Returns:\n",
    "            Dataloader for validation phase.\n",
    "        \"\"\"\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_set, self.batch_size\n",
    "        )\n",
    "\n",
    "\n",
    "class ImageNetDataModule(BaseDataModule):\n",
    "    def __init__(self, data_dir: str, input_domain: str, batch_size: int = 32) -> None:\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.input_domain = input_domain\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self):\n",
    "        normalize = transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "        )\n",
    "        traindir = os.path.join(self.data_dir, 'train')\n",
    "        valdir = os.path.join(self.data_dir, 'val')\n",
    "\n",
    "        \n",
    "\n",
    "        self.train_set = ImageFolder(\n",
    "            traindir,\n",
    "            transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomResizedCrop(224),\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    *domain_transfrom,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        self.val_set = ImageFolder(\n",
    "            valdir,\n",
    "            transforms.Compose(\n",
    "                [\n",
    "                    transforms.Resize(256),\n",
    "                    transforms.CenterCrop(224),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                    *domain_transfrom,\n",
    "                ]\n",
    "            ),\n",
    "        )\n",
    "\n",
    "\n",
    "class MNISTDataModule(BaseDataModule):\n",
    "    \n",
    "    def __init__(self, domain: str, batch_size: int = 32) -> None:\n",
    "        super().__init__(domain=domain)\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        datasets.MNIST(root='MNIST', download=True, train=True)\n",
    "        datasets.MNIST(root='MNIST', download=True, train=False)\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        tensor_transform = transforms.ToTensor()\n",
    "\n",
    "        self.test_set = datasets.MNIST(\n",
    "            root='MNIST', download=True, train=False,\n",
    "            transform= transforms.Compose([tensor_transform, self.domain_transform]))\n",
    "\n",
    "        data_set = datasets.MNIST(\n",
    "            root='MNIST', download=True, train=True,\n",
    "            transform= transforms.Compose([tensor_transform, self.domain_transform]))\n",
    "        \n",
    "        # use 20% of training data for validation\n",
    "        train_set_size = int(len(data_set) * 0.8)\n",
    "        valid_set_size = len(data_set) - train_set_size\n",
    "        \n",
    "        self.train_set, self.val_set = torch.utils.data.random_split(\n",
    "            data_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "def CFAR10DataModule(BaseDataModule):\n",
    "\n",
    "    def __init__(self, domain: str, batch_szie: int = 32) -> None:\n",
    "        super().__init__(domain=domain)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # download\n",
    "        datasets.CIFAR10(root='CIFAR10', download=True, train=True)\n",
    "        datasets.CIFAR10(root='CIFAR10', download=True, train=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b5f892-0e60-42e8-98b9-c809cb931f6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import  defaultdict\n",
    "class FindSumPairs:\n",
    "\n",
    "    def __init__(self, nums1: List[int], nums2: List[int]):\n",
    "        self.dict1 = defaultdict(int)\n",
    "        for num in nums1:\n",
    "            self.dict1[num] += 1\n",
    "        self.dict2 = defaultdict(int)\n",
    "        for num in nums2:\n",
    "            self.dict2[num] += 1\n",
    "        self.nums2 = nums2\n",
    "\n",
    "    def add(self, index: int, val: int) -> None:\n",
    "        print(self.dict2)\n",
    "        self.dict2[self.nums2[index] + val] += 1\n",
    "        self.dict2[self.nums2[index]] -= 1 \n",
    "\n",
    "    def count(self, tot: int) -> int:\n",
    "        print(self.dict2)\n",
    "        ret = 0\n",
    "        for num, count in self.dict1.items():\n",
    "            ret += (count * self.dict2[tot - num])\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7de73f-7ebb-444d-9f7b-33fb333e7583",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_set = datasets.MNIST(root=\"MNIST\", download=True, train=True, transform=transform)\n",
    "\n",
    "# use 20% of training data for validation\n",
    "train_set_size = int(len(train_set) * 0.8)\n",
    "valid_set_size = len(train_set) - train_set_size\n",
    "\n",
    "# split the train set into two\n",
    "seed = torch.Generator().manual_seed(42)\n",
    "train_set, valid_set = torch.utils.data.random_split(train_set, [train_set_size, valid_set_size], generator=seed)\n",
    "\n",
    "test_set = datasets.MNIST(root=\"MNIST\", download=True, train=False, transform=transform)\n",
    "train_loader = DataLoader(train_set, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d5e9961-96b9-43bd-8caf-e92dc52d0d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MLP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m net \u001b[39m=\u001b[39m MLP([\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m], [\u001b[39m10\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m criterion \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0.00001\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
     ]
    }
   ],
   "source": [
    "net = MLP([28, 28], [10])\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "lr = 0.00001\n",
    "num_classes = 10\n",
    "step_size = 300\n",
    "scheduler_class = torch.optim.lr_scheduler.StepLR\n",
    "optimizer_class = torch.optim.Adam\n",
    "lit_model = LitClassificationModel(\n",
    "    net, lr, num_classes, criterion,\n",
    "    optimizer_class, step_size, scheduler_class\n",
    ")\n",
    "datamodule = MNISTDataModule('freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6affa9-a35d-4a4e-8dff-48a8ca830016",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/reghbai7/repos/domain-exploration/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'lit_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m L\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/reghbai7/repos/domain-exploration/notebooks/domain.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model\u001b[39m=\u001b[39mlit_model, datamodule\u001b[39m=\u001b[39mdatamodule)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lit_model' is not defined"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=10)\n",
    "trainer.fit(model=lit_model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce6d81da-4bc2-40a8-a020-348de8cea8f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reghbai7/repos/pyramidal/venv/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "192ddb7259a3458f8c4a3ead78fde28c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |                                                                                                    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "           acc              0.0949999988079071\n",
      "        test_loss           2.3090193271636963\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 2.3090193271636963, 'acc': 0.0949999988079071}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=lit_model, dataloaders=DataLoader(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f22526d-7cc9-4932-a0b7-7b696cd7b9a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in datasets.MNIST(root='MNIST', download=True, train=True):\n",
    "    transforms.Compose([transforms.ToTensor(), SimpleFreqSpace(), SimpleComplex2Vec()])(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644805d-4409-4df7-960f-646f0ca60e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d98870f8-d986-4eac-b466-014233a60727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms.ToTensor()(image).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bfd81ba-d01a-46cb-85a0-2c4c540f324b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[386.2591+0.0000j,   3.3026+4.0290j,  -1.0987-14.9681j,\n",
       "           3.3526+11.1068j,   2.9130-7.9891j,   4.3957-10.9407j,\n",
       "          -7.0289+0.7846j,  -0.5100-0.4097j,  -2.1512+9.7052j,\n",
       "          -3.0485-2.7976j,   5.1947-1.9727j,   3.0755+6.0210j,\n",
       "           2.3637+4.5284j,  -0.9809+11.7598j,   3.5509+0.0000j]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 28\n",
    "m = 28\n",
    "x = transforms.Compose([SimpleFreqSpace()])(torch.rand([1, n, m]))\n",
    "x[:, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4bf65034-a9af-43a6-a8ef-f62f8b6be8b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 14, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    ".shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d5e85bc-4915-4829-a3ac-a699ae9714cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 13, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.view_as_real(x[:, :, 1:-1]).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1fb48f2a-986f-4fce-860b-2e79f38e1928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000],\n",
       "         [ 7.9913],\n",
       "         [-5.1613],\n",
       "         [ 0.2433],\n",
       "         [-0.7990],\n",
       "         [ 4.1034],\n",
       "         [ 4.6483],\n",
       "         [ 1.1475],\n",
       "         [-7.2214],\n",
       "         [ 1.2170],\n",
       "         [ 3.7070],\n",
       "         [ 4.6968],\n",
       "         [-6.3063],\n",
       "         [-8.0839],\n",
       "         [ 0.0000]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[:, : n // 2 + 1, 0:1].imag"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
